{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP+PAK8zS24KY4REqHvrw1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/group-geopulse/GeoPulse/blob/main/project_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM0bTtoU1MxO"
      },
      "outputs": [],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Replace with your NewsAPI key\n",
        "NEWS_API_KEY = \"1b72c33507fe47d69e6f5d9976bf9388\"\n",
        "NEWS_API_URL = \"https://newsapi.org/v2/top-headlines\"\n",
        "\n",
        "def get_news_headlines():\n",
        "    \"\"\"Fetches top news headlines using NewsAPI.\"\"\"\n",
        "    params = {\n",
        "        \"country\": \"us\",  # Change to your preferred country code\n",
        "        \"apiKey\": NEWS_API_KEY\n",
        "    }\n",
        "    response = requests.get(NEWS_API_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        articles = data.get(\"articles\", [])\n",
        "\n",
        "        if not articles:\n",
        "            print(\"No articles found.\")\n",
        "            return None\n",
        "\n",
        "        # Get the first article\n",
        "        first_article = articles[0]\n",
        "        title = first_article[\"title\"]\n",
        "        url = first_article[\"url\"]\n",
        "\n",
        "        print(f\"Headline: {title}\\nURL: {url}\\n\")\n",
        "        return url\n",
        "    else:\n",
        "        print(\"Failed to fetch news.\")\n",
        "        return None\n",
        "\n",
        "def scrape_article_content(url):\n",
        "    \"\"\"Scrapes article content from the provided URL using BeautifulSoup.\"\"\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract all paragraph texts as article content\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
        "\n",
        "        print(\"\\nArticle Content:\\n\")\n",
        "        print(article_text[:500])  # Show only first 500 chars for brevity\n",
        "    else:\n",
        "        print(\"Failed to fetch article content.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    article_url = get_news_headlines()\n",
        "    if article_url:\n",
        "        scrape_article_content(\"https://en.wikipedia.org/wiki/English_Wikipedia\")"
      ],
      "metadata": {
        "id": "i8b_ppVP1Rz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OV7Sd3O41pTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}